# test_deepseek.py
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

print("ğŸš€ Chargement du modÃ¨le DeepSeek Coder (7B instruct)...")
model_id = "deepseek-ai/deepseek-coder-7b-instruct-v1.5"

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    device_map="auto",       # utilise "cpu" si pas de GPU
    torch_dtype="auto"
)

llm = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=2048,
    temperature=0
)

prompt = """Tu es un assistant SQL. GÃ©nÃ¨re une requÃªte SQL correcte.

Question : Combien d'Ã©lÃ¨ves ont payÃ© par chÃ¨que ?
==>"""

print("ğŸ§  DeepSeek gÃ©nÃ¨re une rÃ©ponse...")
response = llm(prompt)[0]["generated_text"]
print("\nğŸŸ© RÃ©ponse gÃ©nÃ©rÃ©e :\n")
print(response)
